{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_sklearn_2016320145.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtSkrg1OgmUA",
        "outputId": "b982af41-1fd8-4c70-bd7b-31a2164b4eae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sl\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "!git clone https://github.com/wookylee/ML_for_spam_text_selection.git\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "filename = '/content/drive/My Drive/기계학습/train.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "\n",
        "import nltk as nlp\n",
        "import re\n",
        "description_list = []\n",
        "for description in data['message']:\n",
        "    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
        "    description = description.lower()   \n",
        "    description = nlp.word_tokenize(description)\n",
        "    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma = nlp.WordNetLemmatizer()\n",
        "    description = [ lemma.lemmatize(word) for word in description]\n",
        "    description = \" \".join(description)\n",
        "    description_list.append(description)\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "max_features = 1000 #We use the most common word\n",
        "count_vectorizer = CountVectorizer( max_features = max_features ,stop_words = \"english\")\n",
        "sparce_matrix = count_vectorizer.fit_transform(description_list).toarray()\n",
        "print(\"the most using {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n",
        "features = count_vectorizer.get_feature_names()\n",
        "print(count_vectorizer.get_feature_names())\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
        "\n",
        "def print_score(classifier,X_train,y_train,X_test,y_test,train=True):\n",
        "    if train == True:\n",
        "        print(\"Training results:\\n\")\n",
        "        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_train,classifier.predict(X_train))))\n",
        "        print(sum(classifier.predict(X_train)))\n",
        "        #print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))\n",
        "        #print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_train,classifier.predict(X_train))))\n",
        "        res = cross_val_score(classifier, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n",
        "        print(res)\n",
        "        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n",
        "        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n",
        "    elif train == False:\n",
        "        print(\"Test results:\\n\")\n",
        "        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_test,classifier.predict(X_test))))\n",
        "        #print('Classification Report:\\n{}\\n'.format(classification_report(y_test,classifier.predict(X_test))))\n",
        "        #print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_test,classifier.predict(X_test))))\n",
        "\n",
        "\n",
        "#We separate our data is train and test\n",
        "y = data.iloc[:,1].values   # male or female classes\n",
        "\n",
        "x = sparce_matrix\n",
        "print(sparce_matrix)\n",
        "print(len(sparce_matrix))\n",
        "print(len(sparce_matrix[0]))\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n",
        "y\n",
        "\n",
        "#We make model for predict\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train,y_train)\n",
        "print(\"the accuracy of our model: {}\".format(nb.score(X_test,y_test)))\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "\n",
        "knn = KNN()\n",
        "knn.fit(X_train,y_train)\n",
        "print('----------KNeighborClassifier----------')\n",
        "print(\"our accuracy is: {}\".format(knn.score(X_test,y_test)))\n",
        "print_score(knn,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(knn,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC(kernel='rbf',random_state=42)\n",
        "\n",
        "svc.fit(X_train,y_train)\n",
        "print('----------SupportVectorClassifier----------')\n",
        "print(\"our accuracy is: {}\".format(svc.score(X_test,y_test)))\n",
        "print_score(svc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(svc,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)\n",
        "rfc.fit(X_train, y_train)\n",
        "print('----------RandomForestClassifier----------')\n",
        "print(\"our accuracy is: {}\".format(rfc.score(X_test,y_test)))\n",
        "print_score(rfc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(rfc,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(max_iter = 200)\n",
        "lr.fit(X_train,y_train)\n",
        "#lr.fit(x,y)\n",
        "print('----------LogisticRegression----------')\n",
        "print(\"our accuracy is: {}\".format(lr.score(X_test,y_test)))\n",
        "print_score(lr,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(lr,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier as DT\n",
        "\n",
        "dtc = DT(criterion='entropy',random_state=42)\n",
        "dtc.fit(X_train,y_train)\n",
        "print('----------DecisionTreeClassifier----------' )\n",
        "print(\"our accuracy is: {}\".format(dtc.score(X_test,y_test)))\n",
        "print_score(dtc,X_train,y_train,X_test,y_test,train=True)\n",
        "print_score(dtc,X_train,y_train,X_test,y_test,train=False)\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n",
        "from IPython.display import Image\n",
        "\n",
        "list_alpha = np.arange(1/100000, 20, 0.11)\n",
        "score_train = np.zeros(len(list_alpha))\n",
        "score_test = np.zeros(len(list_alpha))\n",
        "recall_test = np.zeros(len(list_alpha))\n",
        "precision_test= np.zeros(len(list_alpha))\n",
        "count = 0\n",
        "for alpha in list_alpha:\n",
        "    bayes = naive_bayes.MultinomialNB(alpha=alpha)\n",
        "    bayes.fit(X_train, y_train)\n",
        "    score_train[count] = bayes.score(X_train, y_train)\n",
        "    score_test[count]= bayes.score(X_test, y_test)\n",
        "    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n",
        "    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n",
        "    count = count + 1 \n",
        "matrix = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\n",
        "models = pd.DataFrame(data = matrix, columns = \n",
        "             ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\n",
        "print(models.head(n=10))\n",
        "\n",
        "best_index = models['Test Precision'].idxmax()\n",
        "models.iloc[best_index, :]\n",
        "models[models['Test Precision']==1].head(n=5)\n",
        "\n",
        "best_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\n",
        "print('###############################')\n",
        "print(best_index)\n",
        "print(list_alpha[best_index])\n",
        "print('###############################')\n",
        "bayes = naive_bayes.MultinomialNB(alpha=list_alpha[best_index])\n",
        "bayes.fit(X_train, y_train)\n",
        "models.iloc[best_index, :]\n",
        "\n",
        "\n",
        "# filename = '/content/drive/My Drive/기계학습/RFC.pkl'\n",
        "# pickle.dump(rfc, open(filename, 'wb'))\n",
        "\n",
        "# filename = '/content/drive/My Drive/기계학습/Bayes.pkl'\n",
        "# pickle.dump(bayes, open(filename, 'wb'))\n",
        "\n",
        "# filename = '/content/drive/My Drive/기계학습/LR.pkl'\n",
        "# pickle.dump(lr, open(filename, 'wb'))\n",
        "\n",
        "# filename = '/content/drive/My Drive/기계학습/features.pkl'\n",
        "# pickle.dump(count_vectorizer.get_feature_names(), open(filename, 'wb'))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Cloning into 'ML_for_spam_text_selection'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "Mounted at /content/drive/\n",
            "the most using 1000 words: ['abiola', 'able', 'abt', 'ac', 'access', 'account', 'actually', 'add', 'address', 'admirer', 'aft', 'afternoon', 'age', 'ago', 'ah', 'ahead', 'aight', 'air', 'al', 'alex', 'alright', 'alrite', 'amp', 'angry', 'answer', 'anymore', 'anytime', 'apply', 'appreciate', 'april', 'ar', 'ard', 'area', 'arrive', 'asap', 'ask', 'asked', 'asking', 'attempt', 'auction', 'available', 'ave', 'await', 'award', 'awarded', 'away', 'awesome', 'babe', 'baby', 'bad', 'bag', 'bak', 'balance', 'bank', 'bath', 'bathe', 'battery', 'bb', 'bcoz', 'beautiful', 'bed', 'bedroom', 'begin', 'believe', 'belly', 'best', 'better', 'bid', 'big', 'bird', 'birthday', 'bit', 'black', 'blood', 'blue', 'bluetooth', 'body', 'bold', 'bonus', 'boo', 'book', 'booked', 'bored', 'bother', 'bout', 'box', 'boy', 'boytoy', 'break', 'bring', 'brings', 'brother', 'bslvyl', 'bt', 'btw', 'bugis', 'bus', 'busy', 'buy', 'buzz', 'bye', 'cal', 'called', 'caller', 'calling', 'camcorder', 'came', 'camera', 'car', 'card', 'care', 'carlos', 'case', 'cash', 'catch', 'cause', 'cc', 'cd', 'chance', 'change', 'changed', 'charge', 'charged', 'charity', 'chat', 'cheap', 'check', 'checking', 'cheer', 'chennai', 'chikku', 'child', 'choose', 'christmas', 'cine', 'cinema', 'claim', 'class', 'clean', 'close', 'club', 'code', 'coffee', 'coin', 'cold', 'collect', 'collection', 'college', 'colour', 'com', 'come', 'comin', 'coming', 'comp', 'company', 'complimentary', 'computer', 'confirm', 'congrats', 'congratulation', 'contact', 'content', 'cool', 'copy', 'correct', 'cost', 'couple', 'course', 'cover', 'coz', 'cr', 'crave', 'crazy', 'credit', 'cum', 'currently', 'custcare', 'customer', 'cut', 'cute', 'cuz', 'da', 'dad', 'daddy', 'damn', 'darlin', 'darren', 'dat', 'date', 'dating', 'day', 'dead', 'deal', 'dear', 'decide', 'decided', 'decimal', 'delivery', 'den', 'depends', 'dey', 'did', 'didn', 'didnt', 'die', 'difficult', 'digital', 'din', 'dinner', 'direct', 'dis', 'discount', 'disturb', 'dnt', 'doctor', 'doe', 'doesn', 'doesnt', 'dogging', 'doin', 'doing', 'don', 'dont', 'door', 'double', 'download', 'draw', 'dream', 'drink', 'drive', 'driving', 'drop', 'drug', 'dude', 'dun', 'dunno', 'dvd', 'earlier', 'early', 'easy', 'eat', 'eatin', 'eating', 'ec', 'eh', 'em', 'email', 'end', 'ending', 'energy', 'england', 'enjoy', 'enter', 'entered', 'entry', 'especially', 'eve', 'evening', 'ex', 'exam', 'excellent', 'excuse', 'expires', 'extra', 'eye', 'face', 'facebook', 'fact', 'fall', 'family', 'fancy', 'fantastic', 'far', 'fast', 'fat', 'father', 'fault', 'fb', 'feel', 'feeling', 'felt', 'fetch', 'figure', 'file', 'film', 'final', 'finally', 'fine', 'finish', 'finished', 'fixed', 'flight', 'flirt', 'fone', 'food', 'forever', 'forget', 'forgot', 'forward', 'forwarded', 'fr', 'free', 'freemsg', 'frens', 'fri', 'friday', 'friend', 'friendship', 'frm', 'frnd', 'frnds', 'fuck', 'fucking', 'fun', 'funny', 'gal', 'game', 'gas', 'gave', 'gbp', 'gd', 'gettin', 'getting', 'getzed', 'gift', 'girl', 'glad', 'god', 'goin', 'going', 'gon', 'gone', 'good', 'goodmorning', 'got', 'gr', 'great', 'grin', 'gt', 'guaranteed', 'gud', 'guess', 'guy', 'gym', 'ha', 'haf', 'haha', 'hair', 'half', 'hand', 'happen', 'happened', 'happening', 'happens', 'happiness', 'happy', 'hard', 'hav', 'haven', 'havent', 'having', 'head', 'hear', 'heard', 'heart', 'hee', 'hello', 'help', 'hey', 'hg', 'hi', 'high', 'hit', 'hl', 'hmm', 'hmmm', 'hold', 'holiday', 'home', 'honey', 'hook', 'hop', 'hope', 'hoping', 'hospital', 'hot', 'hotel', 'hour', 'house', 'hows', 'hp', 'hr', 'http', 'huh', 'hungry', 'hurry', 'hurt', 'hw', 'ice', 'id', 'idea', 'identifier', 'il', 'ill', 'im', 'imagine', 'imma', 'immediately', 'important', 'india', 'indian', 'info', 'information', 'invited', 'ipod', 'ish', 'isn', 'issue', 'ive', 'jay', 'job', 'john', 'join', 'joined', 'joke', 'joy', 'jus', 'just', 'juz', 'kallis', 'kate', 'kb', 'kick', 'kid', 'kind', 'kinda', 'king', 'kiss', 'knew', 'know', 'knw', 'la', 'lady', 'land', 'landline', 'laptop', 'lar', 'late', 'later', 'latest', 'laugh', 'lazy', 'ldn', 'le', 'lead', 'leaf', 'leave', 'leaving', 'lect', 'left', 'leh', 'lei', 'lesson', 'let', 'liao', 'library', 'life', 'light', 'like', 'line', 'link', 'listen', 'little', 'live', 'll', 'load', 'loan', 'location', 'log', 'logo', 'lol', 'long', 'look', 'looking', 'lor', 'lose', 'lost', 'lot', 'love', 'loved', 'lovely', 'lover', 'loverboy', 'loving', 'loyalty', 'lt', 'luck', 'lucky', 'lunch', 'luv', 'ma', 'mah', 'mail', 'make', 'making', 'man', 'march', 'married', 'match', 'mate', 'matter', 'max', 'maximize', 'mayb', 'maybe', 'mean', 'meant', 'meet', 'meeting', 'meh', 'member', 'men', 'message', 'met', 'mid', 'midnight', 'min', 'mind', 'minute', 'miss', 'missed', 'missing', 'mistake', 'mm', 'mo', 'moan', 'mob', 'mobile', 'mobileupd', 'mode', 'mom', 'moment', 'mon', 'monday', 'money', 'month', 'morning', 'motorola', 'movie', 'mp', 'mr', 'mrng', 'mrt', 'msg', 'mths', 'mu', 'mum', 'music', 'muz', 'na', 'nah', 'naked', 'national', 'naughty', 'nd', 'near', 'need', 'net', 'network', 'neva', 'new', 'news', 'ni', 'nice', 'nigeria', 'night', 'nite', 'noe', 'nokia', 'noon', 'nope', 'norm', 'normal', 'notice', 'nt', 'ntt', 'num', 'number', 'nxt', 'nyt', 'offer', 'office', 'oh', 'ok', 'okay', 'okie', 'old', 'omw', 'online', 'oops', 'open', 'operator', 'opinion', 'opt', 'optout', 'orange', 'orchard', 'order', 'oredi', 'oso', 'outside', 'pa', 'pack', 'page', 'paid', 'pain', 'paper', 'parent', 'park', 'partner', 'party', 'pas', 'password', 'past', 'pay', 'paying', 'pc', 'people', 'person', 'pete', 'phone', 'photo', 'pic', 'pick', 'picking', 'picture', 'pizza', 'place', 'plan', 'play', 'player', 'plenty', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'point', 'poly', 'polys', 'poor', 'post', 'pound', 'power', 'pple', 'ppm', 'pray', 'press', 'pretty', 'price', 'princess', 'private', 'prize', 'prob', 'probably', 'problem', 'process', 'project', 'promise', 'pub', 'putting', 'question', 'quit', 'quite', 'quiz', 'rain', 'rakhesh', 'rate', 'rd', 'reach', 'reached', 'read', 'reading', 'ready', 'real', 'really', 'realy', 'reason', 'receipt', 'receive', 'recently', 'red', 'redeemed', 'reference', 'registered', 'remember', 'remove', 'rent', 'rental', 'replied', 'reply', 'replying', 'request', 'rest', 'return', 'reveal', 'right', 'ring', 'ringtone', 'ringtones', 'rite', 'road', 'rock', 'room', 'roommate', 'rose', 'round', 'row', 'rply', 'run', 'sad', 'sae', 'safe', 'said', 'sale', 'sat', 'saturday', 'savamob', 'save', 'saw', 'say', 'saying', 'sch', 'school', 'sea', 'search', 'second', 'secret', 'seeing', 'seen', 'selected', 'sell', 'semester', 'send', 'sending', 'sense', 'sent', 'service', 'set', 'sex', 'sexy', 'shall', 'shirt', 'shit', 'shop', 'shopping', 'short', 'shower', 'shuhui', 'si', 'sick', 'sigh', 'silent', 'simple', 'single', 'sir', 'sister', 'sitting', 'situation', 'sk', 'slave', 'sleep', 'sleeping', 'slow', 'sm', 'small', 'smile', 'smiling', 'smoke', 'smth', 'snow', 'somebody', 'song', 'soon', 'sorry', 'sort', 'sound', 'space', 'speak', 'special', 'specially', 'spend', 'spent', 'sport', 'spree', 'st', 'stand', 'star', 'start', 'started', 'starting', 'statement', 'station', 'stay', 'std', 'stop', 'store', 'story', 'straight', 'street', 'strong', 'study', 'stuff', 'stupid', 'sub', 'suite', 'summer', 'sun', 'sunday', 'support', 'supposed', 'sure', 'surprise', 'sweet', 'swing', 'ta', 'taking', 'talk', 'talking', 'tampa', 'tc', 'tea', 'teach', 'team', 'teasing', 'tel', 'tell', 'telling', 'tenerife', 'term', 'test', 'text', 'th', 'thank', 'thanks', 'thanx', 'thats', 'thing', 'think', 'thinkin', 'thinking', 'thk', 'thnk', 'tho', 'thought', 'throw', 'tht', 'thurs', 'ticket', 'til', 'till', 'time', 'tired', 'title', 'tmr', 'today', 'told', 'tomo', 'tomorrow', 'tone', 'tonight', 'tonite', 'took', 'tot', 'touch', 'town', 'train', 'training', 'treat', 'tried', 'trip', 'trouble', 'true', 'trust', 'truth', 'try', 'trying', 'tuesday', 'turn', 'tv', 'txt', 'txting', 'txts', 'type', 'ugh', 'uk', 'umma', 'uncle', 'understand', 'unless', 'unlimited', 'unsubscribe', 'update', 'ur', 'urgent', 'use', 'used', 'usf', 'using', 'usual', 'usually', 'valentine', 'valid', 'valued', 've', 'video', 'visit', 'vl', 'voice', 'voucher', 'wa', 'wait', 'waiting', 'wake', 'walk', 'wan', 'wana', 'want', 'wanted', 'wap', 'warm', 'wasn', 'wat', 'watch', 'watching', 'water', 'way', 'wc', 'weather', 'wed', 'weed', 'week', 'weekend', 'weekly', 'weight', 'welcome', 'wen', 'went', 'whats', 'wid', 'wif', 'wife', 'wil', 'willing', 'win', 'wine', 'winner', 'wish', 'wit', 'wiv', 'wk', 'woke', 'woman', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'word', 'work', 'workin', 'working', 'world', 'worried', 'worry', 'worth', 'wot', 'wow', 'wq', 'wrong', 'wun', 'www', 'wx', 'xmas', 'xx', 'xxx', 'xy', 'ya', 'yahoo', 'yar', 'yeah', 'year', 'yep', 'yes', 'yest', 'yesterday', 'yo', 'yr', 'yup']\n",
            "['abiola', 'able', 'abt', 'ac', 'access', 'account', 'actually', 'add', 'address', 'admirer', 'aft', 'afternoon', 'age', 'ago', 'ah', 'ahead', 'aight', 'air', 'al', 'alex', 'alright', 'alrite', 'amp', 'angry', 'answer', 'anymore', 'anytime', 'apply', 'appreciate', 'april', 'ar', 'ard', 'area', 'arrive', 'asap', 'ask', 'asked', 'asking', 'attempt', 'auction', 'available', 'ave', 'await', 'award', 'awarded', 'away', 'awesome', 'babe', 'baby', 'bad', 'bag', 'bak', 'balance', 'bank', 'bath', 'bathe', 'battery', 'bb', 'bcoz', 'beautiful', 'bed', 'bedroom', 'begin', 'believe', 'belly', 'best', 'better', 'bid', 'big', 'bird', 'birthday', 'bit', 'black', 'blood', 'blue', 'bluetooth', 'body', 'bold', 'bonus', 'boo', 'book', 'booked', 'bored', 'bother', 'bout', 'box', 'boy', 'boytoy', 'break', 'bring', 'brings', 'brother', 'bslvyl', 'bt', 'btw', 'bugis', 'bus', 'busy', 'buy', 'buzz', 'bye', 'cal', 'called', 'caller', 'calling', 'camcorder', 'came', 'camera', 'car', 'card', 'care', 'carlos', 'case', 'cash', 'catch', 'cause', 'cc', 'cd', 'chance', 'change', 'changed', 'charge', 'charged', 'charity', 'chat', 'cheap', 'check', 'checking', 'cheer', 'chennai', 'chikku', 'child', 'choose', 'christmas', 'cine', 'cinema', 'claim', 'class', 'clean', 'close', 'club', 'code', 'coffee', 'coin', 'cold', 'collect', 'collection', 'college', 'colour', 'com', 'come', 'comin', 'coming', 'comp', 'company', 'complimentary', 'computer', 'confirm', 'congrats', 'congratulation', 'contact', 'content', 'cool', 'copy', 'correct', 'cost', 'couple', 'course', 'cover', 'coz', 'cr', 'crave', 'crazy', 'credit', 'cum', 'currently', 'custcare', 'customer', 'cut', 'cute', 'cuz', 'da', 'dad', 'daddy', 'damn', 'darlin', 'darren', 'dat', 'date', 'dating', 'day', 'dead', 'deal', 'dear', 'decide', 'decided', 'decimal', 'delivery', 'den', 'depends', 'dey', 'did', 'didn', 'didnt', 'die', 'difficult', 'digital', 'din', 'dinner', 'direct', 'dis', 'discount', 'disturb', 'dnt', 'doctor', 'doe', 'doesn', 'doesnt', 'dogging', 'doin', 'doing', 'don', 'dont', 'door', 'double', 'download', 'draw', 'dream', 'drink', 'drive', 'driving', 'drop', 'drug', 'dude', 'dun', 'dunno', 'dvd', 'earlier', 'early', 'easy', 'eat', 'eatin', 'eating', 'ec', 'eh', 'em', 'email', 'end', 'ending', 'energy', 'england', 'enjoy', 'enter', 'entered', 'entry', 'especially', 'eve', 'evening', 'ex', 'exam', 'excellent', 'excuse', 'expires', 'extra', 'eye', 'face', 'facebook', 'fact', 'fall', 'family', 'fancy', 'fantastic', 'far', 'fast', 'fat', 'father', 'fault', 'fb', 'feel', 'feeling', 'felt', 'fetch', 'figure', 'file', 'film', 'final', 'finally', 'fine', 'finish', 'finished', 'fixed', 'flight', 'flirt', 'fone', 'food', 'forever', 'forget', 'forgot', 'forward', 'forwarded', 'fr', 'free', 'freemsg', 'frens', 'fri', 'friday', 'friend', 'friendship', 'frm', 'frnd', 'frnds', 'fuck', 'fucking', 'fun', 'funny', 'gal', 'game', 'gas', 'gave', 'gbp', 'gd', 'gettin', 'getting', 'getzed', 'gift', 'girl', 'glad', 'god', 'goin', 'going', 'gon', 'gone', 'good', 'goodmorning', 'got', 'gr', 'great', 'grin', 'gt', 'guaranteed', 'gud', 'guess', 'guy', 'gym', 'ha', 'haf', 'haha', 'hair', 'half', 'hand', 'happen', 'happened', 'happening', 'happens', 'happiness', 'happy', 'hard', 'hav', 'haven', 'havent', 'having', 'head', 'hear', 'heard', 'heart', 'hee', 'hello', 'help', 'hey', 'hg', 'hi', 'high', 'hit', 'hl', 'hmm', 'hmmm', 'hold', 'holiday', 'home', 'honey', 'hook', 'hop', 'hope', 'hoping', 'hospital', 'hot', 'hotel', 'hour', 'house', 'hows', 'hp', 'hr', 'http', 'huh', 'hungry', 'hurry', 'hurt', 'hw', 'ice', 'id', 'idea', 'identifier', 'il', 'ill', 'im', 'imagine', 'imma', 'immediately', 'important', 'india', 'indian', 'info', 'information', 'invited', 'ipod', 'ish', 'isn', 'issue', 'ive', 'jay', 'job', 'john', 'join', 'joined', 'joke', 'joy', 'jus', 'just', 'juz', 'kallis', 'kate', 'kb', 'kick', 'kid', 'kind', 'kinda', 'king', 'kiss', 'knew', 'know', 'knw', 'la', 'lady', 'land', 'landline', 'laptop', 'lar', 'late', 'later', 'latest', 'laugh', 'lazy', 'ldn', 'le', 'lead', 'leaf', 'leave', 'leaving', 'lect', 'left', 'leh', 'lei', 'lesson', 'let', 'liao', 'library', 'life', 'light', 'like', 'line', 'link', 'listen', 'little', 'live', 'll', 'load', 'loan', 'location', 'log', 'logo', 'lol', 'long', 'look', 'looking', 'lor', 'lose', 'lost', 'lot', 'love', 'loved', 'lovely', 'lover', 'loverboy', 'loving', 'loyalty', 'lt', 'luck', 'lucky', 'lunch', 'luv', 'ma', 'mah', 'mail', 'make', 'making', 'man', 'march', 'married', 'match', 'mate', 'matter', 'max', 'maximize', 'mayb', 'maybe', 'mean', 'meant', 'meet', 'meeting', 'meh', 'member', 'men', 'message', 'met', 'mid', 'midnight', 'min', 'mind', 'minute', 'miss', 'missed', 'missing', 'mistake', 'mm', 'mo', 'moan', 'mob', 'mobile', 'mobileupd', 'mode', 'mom', 'moment', 'mon', 'monday', 'money', 'month', 'morning', 'motorola', 'movie', 'mp', 'mr', 'mrng', 'mrt', 'msg', 'mths', 'mu', 'mum', 'music', 'muz', 'na', 'nah', 'naked', 'national', 'naughty', 'nd', 'near', 'need', 'net', 'network', 'neva', 'new', 'news', 'ni', 'nice', 'nigeria', 'night', 'nite', 'noe', 'nokia', 'noon', 'nope', 'norm', 'normal', 'notice', 'nt', 'ntt', 'num', 'number', 'nxt', 'nyt', 'offer', 'office', 'oh', 'ok', 'okay', 'okie', 'old', 'omw', 'online', 'oops', 'open', 'operator', 'opinion', 'opt', 'optout', 'orange', 'orchard', 'order', 'oredi', 'oso', 'outside', 'pa', 'pack', 'page', 'paid', 'pain', 'paper', 'parent', 'park', 'partner', 'party', 'pas', 'password', 'past', 'pay', 'paying', 'pc', 'people', 'person', 'pete', 'phone', 'photo', 'pic', 'pick', 'picking', 'picture', 'pizza', 'place', 'plan', 'play', 'player', 'plenty', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'point', 'poly', 'polys', 'poor', 'post', 'pound', 'power', 'pple', 'ppm', 'pray', 'press', 'pretty', 'price', 'princess', 'private', 'prize', 'prob', 'probably', 'problem', 'process', 'project', 'promise', 'pub', 'putting', 'question', 'quit', 'quite', 'quiz', 'rain', 'rakhesh', 'rate', 'rd', 'reach', 'reached', 'read', 'reading', 'ready', 'real', 'really', 'realy', 'reason', 'receipt', 'receive', 'recently', 'red', 'redeemed', 'reference', 'registered', 'remember', 'remove', 'rent', 'rental', 'replied', 'reply', 'replying', 'request', 'rest', 'return', 'reveal', 'right', 'ring', 'ringtone', 'ringtones', 'rite', 'road', 'rock', 'room', 'roommate', 'rose', 'round', 'row', 'rply', 'run', 'sad', 'sae', 'safe', 'said', 'sale', 'sat', 'saturday', 'savamob', 'save', 'saw', 'say', 'saying', 'sch', 'school', 'sea', 'search', 'second', 'secret', 'seeing', 'seen', 'selected', 'sell', 'semester', 'send', 'sending', 'sense', 'sent', 'service', 'set', 'sex', 'sexy', 'shall', 'shirt', 'shit', 'shop', 'shopping', 'short', 'shower', 'shuhui', 'si', 'sick', 'sigh', 'silent', 'simple', 'single', 'sir', 'sister', 'sitting', 'situation', 'sk', 'slave', 'sleep', 'sleeping', 'slow', 'sm', 'small', 'smile', 'smiling', 'smoke', 'smth', 'snow', 'somebody', 'song', 'soon', 'sorry', 'sort', 'sound', 'space', 'speak', 'special', 'specially', 'spend', 'spent', 'sport', 'spree', 'st', 'stand', 'star', 'start', 'started', 'starting', 'statement', 'station', 'stay', 'std', 'stop', 'store', 'story', 'straight', 'street', 'strong', 'study', 'stuff', 'stupid', 'sub', 'suite', 'summer', 'sun', 'sunday', 'support', 'supposed', 'sure', 'surprise', 'sweet', 'swing', 'ta', 'taking', 'talk', 'talking', 'tampa', 'tc', 'tea', 'teach', 'team', 'teasing', 'tel', 'tell', 'telling', 'tenerife', 'term', 'test', 'text', 'th', 'thank', 'thanks', 'thanx', 'thats', 'thing', 'think', 'thinkin', 'thinking', 'thk', 'thnk', 'tho', 'thought', 'throw', 'tht', 'thurs', 'ticket', 'til', 'till', 'time', 'tired', 'title', 'tmr', 'today', 'told', 'tomo', 'tomorrow', 'tone', 'tonight', 'tonite', 'took', 'tot', 'touch', 'town', 'train', 'training', 'treat', 'tried', 'trip', 'trouble', 'true', 'trust', 'truth', 'try', 'trying', 'tuesday', 'turn', 'tv', 'txt', 'txting', 'txts', 'type', 'ugh', 'uk', 'umma', 'uncle', 'understand', 'unless', 'unlimited', 'unsubscribe', 'update', 'ur', 'urgent', 'use', 'used', 'usf', 'using', 'usual', 'usually', 'valentine', 'valid', 'valued', 've', 'video', 'visit', 'vl', 'voice', 'voucher', 'wa', 'wait', 'waiting', 'wake', 'walk', 'wan', 'wana', 'want', 'wanted', 'wap', 'warm', 'wasn', 'wat', 'watch', 'watching', 'water', 'way', 'wc', 'weather', 'wed', 'weed', 'week', 'weekend', 'weekly', 'weight', 'welcome', 'wen', 'went', 'whats', 'wid', 'wif', 'wife', 'wil', 'willing', 'win', 'wine', 'winner', 'wish', 'wit', 'wiv', 'wk', 'woke', 'woman', 'won', 'wonder', 'wonderful', 'wondering', 'wont', 'word', 'work', 'workin', 'working', 'world', 'worried', 'worry', 'worth', 'wot', 'wow', 'wq', 'wrong', 'wun', 'www', 'wx', 'xmas', 'xx', 'xxx', 'xy', 'ya', 'yahoo', 'yar', 'yeah', 'year', 'yep', 'yes', 'yest', 'yesterday', 'yo', 'yr', 'yup']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "4000\n",
            "1000\n",
            "the accuracy of our model: 0.795\n",
            "----------KNeighborClassifier----------\n",
            "our accuracy is: 0.92\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.9431\n",
            "\n",
            "242\n",
            "[0.925      0.91111111 0.91666667 0.93888889 0.90555556 0.92777778\n",
            " 0.95       0.91944444 0.925      0.93055556]\n",
            "Average Accuracy:\t0.9250\n",
            "\n",
            "Standard Deviation:\t0.0124\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.9200\n",
            "\n",
            "----------SupportVectorClassifier----------\n",
            "our accuracy is: 0.98\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.9925\n",
            "\n",
            "418\n",
            "[0.97222222 0.96666667 0.97777778 0.98055556 0.98333333 0.975\n",
            " 0.98055556 0.96944444 0.96111111 0.975     ]\n",
            "Average Accuracy:\t0.9742\n",
            "\n",
            "Standard Deviation:\t0.0066\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.9800\n",
            "\n",
            "----------RandomForestClassifier----------\n",
            "our accuracy is: 0.9875\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.9986\n",
            "\n",
            "440\n",
            "[0.98333333 0.96944444 0.98055556 0.98333333 0.98055556 0.97222222\n",
            " 0.98888889 0.96388889 0.95       0.975     ]\n",
            "Average Accuracy:\t0.9747\n",
            "\n",
            "Standard Deviation:\t0.0109\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.9875\n",
            "\n",
            "----------LogisticRegression----------\n",
            "our accuracy is: 0.985\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.9881\n",
            "\n",
            "404\n",
            "[0.98611111 0.97222222 0.97777778 0.99166667 0.975      0.98055556\n",
            " 0.98333333 0.96944444 0.96111111 0.97777778]\n",
            "Average Accuracy:\t0.9775\n",
            "\n",
            "Standard Deviation:\t0.0083\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.9850\n",
            "\n",
            "----------DecisionTreeClassifier----------\n",
            "our accuracy is: 0.9625\n",
            "Training results:\n",
            "\n",
            "Accuracy Score: 0.9986\n",
            "\n",
            "440\n",
            "[0.97222222 0.95555556 0.95833333 0.96666667 0.95       0.95\n",
            " 0.975      0.94444444 0.93888889 0.96666667]\n",
            "Average Accuracy:\t0.9578\n",
            "\n",
            "Standard Deviation:\t0.0115\n",
            "Test results:\n",
            "\n",
            "Accuracy Score: 0.9625\n",
            "\n",
            "     alpha  Train Accuracy  Test Accuracy  Test Recall  Test Precision\n",
            "0  0.00001        0.984722           0.98     0.927273        0.927273\n",
            "1  0.11001        0.983889           0.98     0.927273        0.927273\n",
            "2  0.22001        0.983889           0.98     0.927273        0.927273\n",
            "3  0.33001        0.983333           0.98     0.927273        0.927273\n",
            "4  0.44001        0.983333           0.98     0.927273        0.927273\n",
            "5  0.55001        0.983889           0.98     0.927273        0.927273\n",
            "6  0.66001        0.983056           0.98     0.927273        0.927273\n",
            "7  0.77001        0.983056           0.98     0.927273        0.927273\n",
            "8  0.88001        0.983056           0.98     0.927273        0.927273\n",
            "9  0.99001        0.983056           0.98     0.927273        0.927273\n",
            "###############################\n",
            "54\n",
            "5.94001\n",
            "###############################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alpha             5.940010\n",
              "Train Accuracy    0.985278\n",
              "Test Accuracy     0.987500\n",
              "Test Recall       0.909091\n",
              "Test Precision    1.000000\n",
              "Name: 54, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUh8Oz1Sb1pM"
      },
      "source": [
        "# filename2 = '/content/drive/My Drive/기계학습/leaderboard_test_file.csv'\n",
        "# eval = pd.read_csv(filename2)\n",
        "\n",
        "# description_list2 = []\n",
        "# for description in eval['message']:\n",
        "#     description = re.sub(\"[^a-zA-Z]\",\" \",description)\n",
        "#     description = description.lower()   \n",
        "#     description = nlp.word_tokenize(description)\n",
        "#     #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
        "#     lemma = nlp.WordNetLemmatizer()\n",
        "#     description = [ lemma.lemmatize(word) for word in description]\n",
        "#     description = \" \".join(description)\n",
        "#     description_list2.append(description)\n",
        "\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer \n",
        "# max_features = 6000 #We use the most common word\n",
        "# count_vectorizer = CountVectorizer( vocabulary = features ,stop_words = \"english\")\n",
        "# sparce_matrix = count_vectorizer.fit_transform(description_list2).toarray()\n",
        "# print(\"the most using {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))\n",
        "# print(count_vectorizer.get_feature_names())\n",
        "# print(sparce_matrix)\n",
        "# print(len(sparce_matrix))\n",
        "# print(len(sparce_matrix[0]))\n",
        "\n",
        "# # if len(sparce_matrix[0])<5995:\n",
        "# #     margin_matrix = np.zeros((len(sparce_matrix),5995-len(sparce_matrix[0])))\n",
        "# #     sparce_matrix = np.hstack((sparce_matrix,margin_matrix))\n",
        "# # print(sparce_matrix)\n",
        "# # print(len(sparce_matrix))\n",
        "# # print(len(sparce_matrix[0]))\n",
        "\n",
        "# x_pred = sparce_matrix.reshape(300,5995,1)\n",
        "\n",
        "# result = regressor.predict_classes(x_pred)\n",
        "\n",
        "# print(result)"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}