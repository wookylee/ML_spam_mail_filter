{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_2016320145.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTHtsnlrR9RN"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns; sns.set()\n",
        "import sklearn as sl\n",
        "import nltk\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "filename = '/content/drive/My Drive/기계학습/train.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "texts = []\n",
        "label=[]\n",
        "\n",
        "for labels in data['label']:\n",
        "    label.append(labels)\n",
        "for message in data['message']:\n",
        "    texts.append(message)\n",
        "# x_raw.append(' '.join(i for i in line.split()[1:]))\n",
        "\n",
        "\n",
        "print(len(label))\n",
        "print(len(texts))\n",
        "\n",
        "from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# number of words used as features\n",
        "max_features = 10000\n",
        "# cut off the words after seeing 500 words in each document(email) \n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found {0} unique words: \".format(len(word_index)))\n",
        "print(sequences)\n",
        "print(word_index)\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "print(\"data shape: \", X.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,label, test_size = 0.1, random_state = 42)\n",
        "x_test = X_test.reshape(400,500,1)\n",
        "x_train = X_train.reshape(3600,500,1)\n",
        "print(X_train)\n",
        "print(len(X_train))\n",
        "print(len(X_train[0]))\n",
        "print(y_train)\n",
        "print(X_train.dtype)\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_rnn = model.fit(X_train, y_train, epochs=10, batch_size=60,validation_split=0.2)\n",
        "\n",
        "acc = history_rnn.history['acc']\n",
        "val_acc = history_rnn.history['val_acc']\n",
        "loss = history_rnn.history['loss']\n",
        "val_loss = history_rnn.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, '-', color='orange', label='training acc')\n",
        "plt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epochs, loss, '-', color='orange', label='training acc')\n",
        "plt.plot(epochs, val_loss,  '-', color='blue', label='validation acc')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "model.summary()\n",
        "pred = model.predict_classes(X_test)\n",
        "acc = model.evaluate(X_test, y_test)\n",
        "proba_rnn = model.predict_proba(X_test)\n",
        "\n",
        "##################################################\n",
        "# ㅈ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N6-ZJ1umysS"
      },
      "source": [
        "filename = '/content/drive/My Drive/기계학습/leaderboard_test_file.csv'\n",
        "result_path='/content/drive/My Drive/기계학습/result.csv'\n",
        "data = pd.read_csv(filename)\n",
        "\n",
        "\n",
        "texts=[]\n",
        "\n",
        "from pandas import DataFrame\n",
        "for message in data['message']:\n",
        "    texts.append(message)\n",
        "\n",
        "print(texts)\n",
        "\n",
        "# number of words used as features\n",
        "max_features = 10000\n",
        "# cut off the words after seeing 500 words in each document(email) \n",
        "maxlen = 500\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.word_index = word_index\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "print(sequences)\n",
        "# word_index = tokenizer.word_index\n",
        "print(\"Found {0} unique words: \".format(len(word_index)))\n",
        "\n",
        "eval = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "print(eval.shape)\n",
        "eval.reshape(300,500,1)\n",
        "model.summary()\n",
        "score = model.predict_classes(eval)\n",
        "print(score)\n",
        "score = pd.DataFrame(score)\n",
        "score.to_csv(result_path,index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}